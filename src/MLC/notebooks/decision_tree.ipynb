{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision tree is a type of machine learning algorithm used for classification and regression tasks. It consists of a tree-like structure where each internal node represents a feature or attribute, each branch represents a decision based on that feature, and each leaf node represents a predicted output.\n",
    "\n",
    "To **train** a decision tree, the algorithm uses a dataset with labeled examples to create the tree structure. It starts with the root node, which includes all the examples, and selects the feature that provides the most information gain to split the data into two subsets. It then repeats this process for each subset until it reaches a stopping criterion, such as a maximum tree depth or minimum number of examples in a leaf node.\n",
    "\n",
    "Once the decision tree is trained, it can be used to **predict** the output for new, unseen examples. To make a prediction, the algorithm starts at the root node and follows the branches based on the values of the input features until it reaches a leaf node. The predicted output for that example is the value associated with the leaf node.\n",
    "\n",
    "Decision trees have several advantages, such as being easy to interpret and visualize, handling both numerical and categorical data, and handling missing values. However, they can also suffer from overfitting if the tree is too complex or if there is noise or outliers in the data. \n",
    "\n",
    "To address this issue, various techniques such as pruning, ensemble methods, and regularization can be used to simplify the decision tree or combine multiple trees to improve generalization performance. Additionally, decision trees may not perform well with highly imbalanced datasets or datasets with many irrelevant features, and they may not be suitable for tasks where the relationships between features and outputs are highly nonlinear or complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "1. Data preparation: less data preparation is required. Do not require features scaling or centering\n",
    "2. Gini impurity: <span style=\"color:red\">$1-\\sum p_{i,k}^2$ </span> ,  $p_{i,k}$ is the ratio of class k instances among the training instances at node i. An alternative is Entropy, which generates very similar trees but gini impurity is faster to compute.\n",
    "3. Classification CART tree cost function weighted sum of Gini impurity on leaves: $\\dfrac{m_{left}}{m}G_{left}+\\dfrac{m_{right}}{m}G_{right}$\n",
    "4. Regression CART tree cost function weighted sum of MSE on leaves\n",
    "5. Hyperparameters that the larger the more overfit: `max_depth`, `max_leaf_nodes`; the smaller the more overfit: `min_sample_split`, `min_sample_leaf`. **Increasing min and reducing max will regularize the tree** \n",
    "6. Computational complexity: training is slow if features and samples are large <span style=\"color:red\">$O(n*mlog(m))$</span>. Prediction is fast.\n",
    "7. <span style=\"color:red\">Decision tree has high variance</span>: The main issue with Decision Tree is that they are very sensitive to small variations in the training data. One solution is to use ensembling to average over multiple trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.n_classes_ = len(np.unique(y))\n",
    "        self.n_features_ = X.shape[1]\n",
    "        self.tree_ = self._grow_tree(X, y)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return [self._predict(inputs) for inputs in X]\n",
    "        \n",
    "    def _gini(self, y):\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        impurity = 1 - np.sum([(count / len(y)) ** 2 for count in counts])\n",
    "        return impurity\n",
    "        \n",
    "    def _best_split(self, X, y):\n",
    "        m = y.size\n",
    "        if m <= 1:\n",
    "            return None, None\n",
    "\n",
    "        # initiate the gini, feature, threshold\n",
    "        num_parent = [np.sum(y == c) for c in range(self.n_classes_)]\n",
    "        best_gini = 1.0 - sum((n / m) ** 2 for n in num_parent) # initiate with gini w/o split\n",
    "        best_idx, best_thr = None, None\n",
    "        \n",
    "        for idx in range(self.n_features_): # for each feature\n",
    "            thresholds, classes = zip(*sorted(zip(X[:, idx], y))) # sort this feature\n",
    "            num_left = [0] * self.n_classes_ # move all instances to right node first. keep track of the make-up of leaves to calculate gini impurity\n",
    "            num_right = num_parent.copy()\n",
    "            for i in range(1, m): # for each value/instance (except for the largest value) of this feature in ascending order move to the left node\n",
    "                c = classes[i - 1] #\n",
    "                num_left[c] += 1\n",
    "                num_right[c] -= 1\n",
    "                gini_left = 1.0 - sum(\n",
    "                    (num_left[x] / i) ** 2 for x in range(self.n_classes_)\n",
    "                )\n",
    "                gini_right = 1.0 - sum(\n",
    "                    (num_right[x] / (m - i)) ** 2 for x in range(self.n_classes_)\n",
    "                )\n",
    "                gini = (i * gini_left + (m - i) * gini_right) / m\n",
    "                if thresholds[i] == thresholds[i - 1]:\n",
    "                    continue\n",
    "                if gini < best_gini:\n",
    "                    best_gini = gini\n",
    "                    best_idx = idx\n",
    "                    best_thr = (thresholds[i] + thresholds[i - 1]) / 2\n",
    "        \n",
    "        return best_idx, best_thr\n",
    "        \n",
    "    def _grow_tree(self, X, y, depth=0): # recursively grow tree by creating node and find the best split(feature & threshold)\n",
    "        num_samples_per_class = [np.sum(y == i) for i in range(self.n_classes_)]\n",
    "        predicted_class = np.argmax(num_samples_per_class)\n",
    "        node = Node(predicted_class=predicted_class)\n",
    "        if depth < self.max_depth:\n",
    "            idx, thr = self._best_split(X, y)\n",
    "            if idx is not None: # there might not need any split\n",
    "                indices_left = X[:, idx] < thr\n",
    "                X_left, y_left = X[indices_left], y[indices_left]\n",
    "                X_right, y_right = X[~indices_left], y[~indices_left]\n",
    "                node.feature_index = idx\n",
    "                node.threshold = thr\n",
    "                node.left = self._grow_tree(X_left, y_left, depth + 1)\n",
    "                node.right = self._grow_tree(X_right, y_right, depth + 1)\n",
    "        return node\n",
    "        \n",
    "    def _predict(self, inputs):\n",
    "        node = self.tree_\n",
    "        while node.left:\n",
    "            if inputs[node.feature_index] < node.threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return node.predicted_class\n",
    "    \n",
    "class Node:\n",
    "    def __init__(self, *, predicted_class):\n",
    "        self.predicted_class = predicted_class\n",
    "        self.feature_index = 0\n",
    "        self.threshold = 0.0 \n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "    def is_leaf_node(self):\n",
    "        return self.left is None and self.right is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classificationtree:\n",
    "    def __init__(self, max_depth):\n",
    "        self.max_depth = max_depth\n",
    "        self._tree = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"update self._tree, wrapper of _grow_tree()\"\"\"\n",
    "        self._n_class = len(np.unique(y))\n",
    "        self._tree = self._grow_tree(X,y,0)\n",
    "\n",
    "    def _gini(self, y):\n",
    "        return 1-np.sum([(np.sum(y[y==i])/len(y))**2 for i in y.unique()])\n",
    "\n",
    "    def _grow_tree(self, X, y, depth):\n",
    "        \"\"\"recursively create nodes, find splits. return root node\"\"\"\n",
    "        num_samples_per_class = [np.sum(y == i) for i in range(self.n_classes_)]\n",
    "        predicted_class = np.argmax(num_samples_per_class)\n",
    "        node = Node(predicted_class=predicted_class) # need to decide predicted_class first before splits\n",
    "        if depth<self.max_depth:\n",
    "            feature, threshold=self._best_split(X,y)\n",
    "            if feature:\n",
    "                left_idx = X[feature]<threshold\n",
    "                right_idx = X[feature]>=threshold\n",
    "                X_left = X[left_idx]\n",
    "                y_left = y[left_idx]\n",
    "                X_right = X[right_idx]\n",
    "                y_right = y[right_idx]\n",
    "                node.left = self._grow_tree(X_left, y_left, depth+1)\n",
    "                node.right = self._grow_tree(X_right, y_right, depth+1)                \n",
    "        return node\n",
    "\n",
    "    def _best_split(self, X, y):\n",
    "        \"\"\"iteratively find the feature and threshold of a node. if no splits are needed, return None\"\"\"\n",
    "        best_gini = self._gini(y)\n",
    "        best_feature=None\n",
    "        best_threshold=None\n",
    "        \n",
    "        for i in range(X.shape[1]):\n",
    "            s = sorted(zip(X[:,i],y))\n",
    "            left = []\n",
    "            right = y.to_list()\n",
    "            for j in range(len(y)):\n",
    "                left.append(s[j,1])\n",
    "                right.remove(s[j,1])\n",
    "                gini_l = self._gini(left)\n",
    "                gini_r = self._gini(right)\n",
    "                gini = ((j+1)*gini_l + (len(y)-j-1)*gini_r)/len(y)\n",
    "                if gini<best_gini:\n",
    "                    best_gini=gini\n",
    "                    best_feature=i\n",
    "                    best_threshold=(s[j,0]+s[j+1,0])/2\n",
    "                    #TODO: edge case of s[j,0]==s[j+1,0]\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    def predict(self, X):\n",
    "        return y_pred\n",
    "\n",
    "    def _predict(self, x):\n",
    "        return y_pred\n",
    "       \n",
    "\n",
    "class Node:\n",
    "    def __init__(self,*,predicted_class):\n",
    "        self.feature=None\n",
    "        self.threshold=None\n",
    "        self.left=None\n",
    "        self.right=None\n",
    "        self.predicted_class=predicted_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the decision tree\n",
    "tree = DecisionTree(max_depth=3)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = tree.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the predictions\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn with Hyperparameter Tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can treat some of the data preparation as hyperparameters. I.e. how to handle outliers, missing features, feature selection, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "data = make_moons(n_samples=10000, noise=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV with grid search to tune hyperparameters\n",
    "from "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
